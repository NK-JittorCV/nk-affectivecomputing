# ç²¾é€‰æƒ…æ„Ÿè®¡ç®—æ–¹æ³•


ğŸ”¥ğŸ”¥ğŸ”¥ **ICML 2025 Spotlight | MART: Masked Affective RepresenTation Learning via Masked Temporal Distribution Distillation**  
<p align="center">
    <img src="../../assets/images/moda.png" width="100%">
</p>

<font size=7><div align='center'>
[[ğŸ“– è®ºæ–‡](https://arxiv.org/abs/2507.04635)]
[[ğŸŒŸ ä»£ç ](https://github.com/KwaiVGI/MODA)]
[[ğŸ›œ æ¨¡å‹](https://huggingface.co/KwaiVGI/MODA)]
[[ğŸ–¼ï¸ æµ·æŠ¥](https://zzcheng.top/assets/pdf/2025_ICML_MODA_poster.pdf)]
[[ğŸ–¼ï¸ å®£è®²](https://zzcheng.top/assets/pdf/2025_ICML_MODA_slide.pdf)]
[[ğŸ‘ ä¸»é¡µ](https://zzcheng.top/MODA/)]
</div></font>  

<font size=7><div align='center' > ä»¥è¯­è¨€ä¸ºä¸­å¿ƒçš„é¢„è®­ç»ƒæœºåˆ¶å¯¼è‡´ç°æœ‰å¤šæ¨¡æ€å¤§æ¨¡å‹å­˜åœ¨æ¨¡æ€åç½®ï¼Œéš¾ä»¥å…³æ³¨åˆ°ç»†ç²’åº¦çš„æƒ…æ„Ÿçº¿ç´¢ã€‚å¿«æ‰‹å¯çµå›¢é˜Ÿä¸å—å¼€å¤§å­¦åœ¨ã€Œå¤šæ¨¡æ€æƒ…æ„Ÿç†è§£ã€é¢†åŸŸå®Œæˆäº†å¼€åˆ›æ€§ç ”ç©¶ï¼ŒæˆåŠŸå®šä½äº†ç°æœ‰å¤šæ¨¡æ€å¤§æ¨¡å‹åœ¨æƒ…æ„Ÿçº¿ç´¢æ•æ‰ä¸­çš„å…³é”®çŸ­æ¿ã€‚ç ”ç©¶å›¢é˜Ÿä»å¤šæ¨¡æ€æ³¨æ„åŠ›æœºåˆ¶çš„ç»´åº¦åˆ‡å…¥ï¼Œæå‡ºäº†æ–°çš„æ¨¡å—åŒ–åŒå·¥æ³¨æ„åŠ›èŒƒå¼ï¼Œå¹¶åŸºäºæ­¤æ„å»ºäº†ä¸€ä¸ªæ¶µç›–æ„ŸçŸ¥ã€è®¤çŸ¥ä¸æƒ…æ„Ÿèƒ½åŠ›çš„å¤šæ¨¡æ€æ¨¡å‹â€˜æ‘©è¾¾ï¼ˆMODAï¼‰â€™ã€‚è¯¥æ¨¡å‹åœ¨é€šç”¨å¯¹è¯ã€çŸ¥è¯†é—®ç­”ã€è¡¨æ ¼å¤„ç†ã€è§†è§‰æ„ŸçŸ¥ã€è®¤çŸ¥åˆ†æå’Œæƒ…æ„Ÿç†è§£ç­‰å…­å¤§ç±»ä»»åŠ¡çš„21ä¸ªåŸºå‡†æµ‹è¯•ä¸­å‡å®ç°äº†æ˜¾è‘—æ€§èƒ½æå‡ã€‚æ­¤å¤–ï¼ŒåŸºäºæ–°çš„æ³¨æ„åŠ›æœºåˆ¶ï¼Œâ€˜æ‘©è¾¾â€™åœ¨è§’è‰²å‰–æä¸è§„åˆ’æ¼”ç»ç­‰äººæœºäº¤äº’åœºæ™¯ä¸­è¡¨ç°å‡ºè‰²ã€‚ç›®å‰ï¼Œè¯¥ç ”ç©¶æˆæœå·²è¢«ICML 2025æ”¶å½•ï¼Œå¹¶è·é€‰ç„¦ç‚¹è®ºæ–‡ï¼ˆSpotlightï¼ŒTop 2.6%ï¼‰ã€‚ âœ¨ </div></font>


---

ğŸ”¥ğŸ”¥ğŸ”¥ **CVPR 2024 | MART: Masked Affective RepresenTation Learning via Masked Temporal Distribution Distillation**  
<p align="center">
    <img src="../../assets/images/mart.png" width="50%">
</p>

<font size=7><div align='center'>
[[ğŸ“– è®ºæ–‡](https://openaccess.thecvf.com/content/CVPR2024/papers/Zhang_MART_Masked_Affective_RepresenTation_Learning_via_Masked_Temporal_Distribution_Distillation_CVPR_2024_paper.pdf)]
[[ğŸŒŸ ä»£ç ](https://github.com/nku-zhichengzhang/MART)]
[[ğŸ–¼ï¸ æµ·æŠ¥](https://zzcheng.top/assets/pdf/2024_CVPR_MART_poster.pdf)]
[[ğŸ‘ ä¸»é¡µ](https://zzcheng.top/MART/)]
[[ğŸ‘ Demo](https://zzcheng.top/projects/VER/)]
</div></font>  

<font size=7><div align='center' > å½“æ ‡ç­¾æåº¦ç¨€ç¼ºæ—¶ï¼Œæ€¥éœ€æŒ–æ˜ä¸€ç§ä½å¼€é”€ã€æ— éœ€æ ‡æ³¨çš„å¤§è§„æ¨¡ç›‘ç£ä¿¡å·ã€‚ä¸ºæ­¤ï¼Œæå‡ºä¸€ç§æ©ç æƒ…æ„Ÿå»ºæ¨¡æ–¹æ³•ï¼Œåˆ©ç”¨è§†é¢‘çš„è¯­è¨€å­¦æƒ…æ„Ÿçº¿ç´¢ï¼Œæ¢å¤æƒ…æ„Ÿçš„æ—¶åŸŸåˆ†å¸ƒæ¥å­¦ä¹ è¾¨åˆ«è¡¨å¾ã€‚ âœ¨ </div></font>


---

ğŸ”¥ğŸ”¥ğŸ”¥ **CVPR 2023 | Weakly Supervised Video Emotion Detection and Prediction via Cross-Modal Temporal Erasing Network**  
<p align="center">
    <img src="../../assets/images/cten.png" width="50%">
</p>

<font size=7><div align='center'>
[[ğŸ“– è®ºæ–‡](https://openaccess.thecvf.com/content/CVPR2023/papers/Zhang_Weakly_Supervised_Video_Emotion_Detection_and_Prediction_via_Cross-Modal_Temporal_CVPR_2023_paper.pdf)]
[[ğŸŒŸ ä»£ç ](https://github.com/nku-zhichengzhang/CTEN)]
[[ğŸ“º è®²è§£è§†é¢‘](https://www.youtube.com/watch?v=ebD_xNQLuCY)]
</div></font>  

<font size=7><div align='center' > ä¸€ç§ç”¨äºè§†é¢‘æƒ…æ„Ÿåˆ†æçš„è·¨æ¨¡æ€æ—¶é—´æ“¦é™¤ç½‘ç»œï¼Œèƒ½å¤Ÿä»¥å¼±ç›‘ç£æ–¹å¼å®šä½å…³é”®å¸§ã€ä¸Šä¸‹æ–‡ä»¥åŠä¸éŸ³é¢‘ç›¸å…³çš„ä¿¡æ¯ âœ¨ </div></font>


---

ğŸ”¥ğŸ”¥ğŸ”¥ **ACM MM 2022 | Temporal Sentiment Localization: Listen and Look in Untrimmed Videos**  
<p align="center">
    <img src="../../assets/images/tsl.png" width="100%">
</p>

<font size=7><div align='center'>
[[ğŸ“– è®ºæ–‡](https://zzcheng.top/assets/pdf/2022_ACMMM_TSL300.pdf)]
[[ğŸŒŸ ä»£ç ](https://github.com/nku-zhichengzhang/TSL300)]
[[ğŸ“º è®²è§£è§†é¢‘](https://www.youtube.com/watch?v=znZZMq6YdBg)]
</div></font>  

<font size=7><div align='center' > ä¸ºè§£å†³å¯†é›†æ ‡æ³¨æ•°æ®é›†çš„é«˜æˆæœ¬é—®é¢˜ï¼Œæå‡ºäº†TSL-Netï¼Œä¸€ç§åˆ©ç”¨å•å¸§ç›‘ç£æ¥å®šä½è§†é¢‘ä¸­çš„æƒ…æ„Ÿçš„å¼±ç›‘ç£æ¡†æ¶ âœ¨ </div></font>

---

ğŸ”¥ğŸ”¥ğŸ”¥ **AAAI 2020 | An End-to-End Visual-Audio Attention Network for Emotion Recognition in User-Generated Videos**  

<font size=7><div align='center'>
[[ğŸ“– è®ºæ–‡](https://arxiv.org/abs/2003.00832)]
[[ğŸŒŸ ä»£ç ](https://github.com/maysonma/VAANet)]
</div></font>  

---

ğŸ”¥ğŸ”¥ğŸ”¥ **TAC 2024 | Looking into Gait for Perceiving Emotions via Bilateral Posture and Movement Graph Convolutional Networks**  

<font size=7><div align='center'>
[[ğŸ“– è®ºæ–‡](https://ieeexplore.ieee.org/document/10433680)]
</div></font>  