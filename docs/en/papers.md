# Awesome-Affective-Computing-Methods




ðŸ”¥ðŸ”¥ðŸ”¥ **CVPR23 | Weakly Supervised Video Emotion Detection and Prediction via Cross-Modal Temporal Erasing Network**  
<p align="center">
    <img src="../../assets/images/cten.png" width="50%">
</p>

<font size=7><div align='center'>
[[ðŸ“– Paper](https://openaccess.thecvf.com/content/CVPR2023/papers/Zhang_Weakly_Supervised_Video_Emotion_Detection_and_Prediction_via_Cross-Modal_Temporal_CVPR_2023_paper.pdf)]
[[ðŸŒŸ GitHub](https://github.com/nku-zhichengzhang/CTEN)]
[[ðŸ“º Video](https://www.youtube.com/watch?v=ebD_xNQLuCY)]
</div></font>  

<font size=7><div align='center' > A cross-modal temporal erasing network for video emotion analysis that locates not only keyframes but also context and audio-related information in a weakly-supervised manner âœ¨ </div></font>


---

ðŸ”¥ðŸ”¥ðŸ”¥ **ACM MM22 | Temporal Sentiment Localization: Listen and Look in Untrimmed Videos**  
<p align="center">
    <img src="../../assets/images/tsl.png" width="100%">
</p>

<font size=7><div align='center'>
[[ðŸ“– Paper](https://github.com/nku-zhichengzhang/TSL300/blob/main/assests/acm22_zzc_videosenti_official.pdf)]
[[ðŸŒŸ GitHub](https://github.com/nku-zhichengzhang/TSL300)]
[[ðŸ“º Video](https://www.youtube.com/watch?v=znZZMq6YdBg)]
</div></font>  

<font size=7><div align='center' > Due to the high cost of labeling a densely annotated dataset, we propose TSL-Net in this work, employing single-frame supervision to localize sentiment in videos âœ¨ </div></font>

---

ðŸ”¥ðŸ”¥ðŸ”¥ **AAAI20 | An End-to-End Visual-Audio Attention Network for Emotion Recognition in User-Generated Videos**  

<font size=7><div align='center'>
[[ðŸ“– Paper](https://arxiv.org/abs/2003.00832)]
[[ðŸŒŸ GitHub](https://github.com/maysonma/VAANet)]
</div></font>  

---

ðŸ”¥ðŸ”¥ðŸ”¥ **TAC | Looking into Gait for Perceiving Emotions via Bilateral Posture and Movement Graph Convolutional Networks**  

<font size=7><div align='center'>
[[ðŸ“– Paper](https://ieeexplore.ieee.org/document/10433680)]
</div></font>  