# Awesome-Affective-Computing-Methods



ğŸ”¥ğŸ”¥ğŸ”¥ **ICML 2025 Spotlight | MART: Masked Affective RepresenTation Learning via Masked Temporal Distribution Distillation**  
<p align="center">
    <img src="../../assets/images/moda.png" width="100%">
</p>

<font size=7><div align='center'>
[[ğŸ“– Paper](https://arxiv.org/abs/2507.04635)]
[[ğŸŒŸ Github](https://github.com/KwaiVGI/MODA)]
[[ğŸ›œ æ¨¡å‹](https://huggingface.co/KwaiVGI/MODA)]
[[ğŸ–¼ï¸ Poster](https://zzcheng.top/assets/pdf/2025_ICML_MODA_poster.pdf)]
[[ğŸ–¼ï¸ å®£è®²](https://zzcheng.top/assets/pdf/2025_ICML_MODA_slide.pdf)]
[[ğŸ‘ Project](https://zzcheng.top/MODA/)]
</div></font>  

<font size=7><div align='center' > The language-centric pretraining mechanism in existing large-scale multimodal models often results in modality bias, making it challenging to capture fine-grained emotional cues. To address this, the Kuaishou Keling team, in collaboration with Nankai University, conducted pioneering research in the field of "multimodal emotion understanding," successfully identifying critical limitations of current multimodal models in detecting emotional signals. The research team introduced a novel modular duplex attention paradigm by focusing on the dimensions of multimodal attention mechanisms. Building on this framework, they developed a multimodal model named MODA, which integrates capabilities in perception, cognition, and emotion understanding. MODA demonstrated substantial performance improvements across 21 benchmark tests spanning six major task categories: general QA, knowledge QA, table & ocr, visual-centric, cognitive analysis, and emotion understanding. Additionally, thanks to the innovative attention mechanism, MODA excelled in human-computer interaction scenarios such as character profiling and planning deduction. This groundbreaking work has been accepted by ICML 2025 and selected as a **Spotlight Paper (Top 2.6%)**. âœ¨ </div></font>


---

ğŸ”¥ğŸ”¥ğŸ”¥ **CVPR 2024 | MART: Masked Affective RepresenTation Learning via Masked Temporal Distribution Distillation**  
<p align="center">
    <img src="../../assets/images/mart.png" width="50%">
</p>

<font size=7><div align='center'>
[[ğŸ“– Paper](https://openaccess.thecvf.com/content/CVPR2024/papers/Zhang_MART_Masked_Affective_RepresenTation_Learning_via_Masked_Temporal_Distribution_Distillation_CVPR_2024_paper.pdf)]
[[ğŸŒŸ Github](https://github.com/nku-zhichengzhang/MART)]
[[ğŸ–¼ï¸ Poster](https://zzcheng.top/assets/pdf/2024_CVPR_MART_poster.pdf)]
[[ğŸ‘ Project](https://zzcheng.top/MART/)]
[[ğŸ‘ Demo](https://zzcheng.top/projects/VER/)]
</div></font>  

<font size=7><div align='center' > When labels are extremely scarce, there is an urgent need to explore a low-cost, annotation-free large-scale supervision signal. To address this, a masked emotion modeling method is proposed, which leverages linguistic emotional cues from videos to reconstruct the temporal distribution of emotions for learning discriminative representations. âœ¨ </div></font>


---

ğŸ”¥ğŸ”¥ğŸ”¥ **CVPR 2023 | Weakly Supervised Video Emotion Detection and Prediction via Cross-Modal Temporal Erasing Network**  
<p align="center">
    <img src="../../assets/images/cten.png" width="50%">
</p>

<font size=7><div align='center'>
[[ğŸ“– Paper](https://openaccess.thecvf.com/content/CVPR2023/papers/Zhang_Weakly_Supervised_Video_Emotion_Detection_and_Prediction_via_Cross-Modal_Temporal_CVPR_2023_paper.pdf)]
[[ğŸŒŸ GitHub](https://github.com/nku-zhichengzhang/CTEN)]
[[ğŸ“º Video](https://www.youtube.com/watch?v=ebD_xNQLuCY)]
</div></font>  

<font size=7><div align='center' > A cross-modal temporal erasing network for video emotion analysis that locates not only keyframes but also context and audio-related information in a weakly-supervised manner âœ¨ </div></font>


---

ğŸ”¥ğŸ”¥ğŸ”¥ **ACM MM 2022 | Temporal Sentiment Localization: Listen and Look in Untrimmed Videos**  
<p align="center">
    <img src="../../assets/images/tsl.png" width="100%">
</p>

<font size=7><div align='center'>
[[ğŸ“– Paper](https://zzcheng.top/assets/pdf/2022_ACMMM_TSL300.pdf)]
[[ğŸŒŸ GitHub](https://github.com/nku-zhichengzhang/TSL300)]
[[ğŸ“º Video](https://www.youtube.com/watch?v=znZZMq6YdBg)]
</div></font>  

<font size=7><div align='center' > Due to the high cost of labeling a densely annotated dataset, we propose TSL-Net in this work, employing single-frame supervision to localize sentiment in videos âœ¨ </div></font>

---

ğŸ”¥ğŸ”¥ğŸ”¥ **AAAI 2020 | An End-to-End Visual-Audio Attention Network for Emotion Recognition in User-Generated Videos**  

<font size=7><div align='center'>
[[ğŸ“– Paper](https://arxiv.org/abs/2003.00832)]
[[ğŸŒŸ GitHub](https://github.com/maysonma/VAANet)]
</div></font>  

---

ğŸ”¥ğŸ”¥ğŸ”¥ **TAC 2024 | Looking into Gait for Perceiving Emotions via Bilateral Posture and Movement Graph Convolutional Networks**  

<font size=7><div align='center'>
[[ğŸ“– Paper](https://ieeexplore.ieee.org/document/10433680)]
</div></font>  