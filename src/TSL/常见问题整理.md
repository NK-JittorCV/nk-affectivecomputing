# PyTorch 转 Jittor 常见问题与解答（FAQ）

## 1. Jittor 支持 PyTorch 的哪些基本操作？

**答：** Jittor 支持大多数常见的张量操作，如 `matmul`, `conv2d`, `relu`, `batchnorm`, `dropout`, `softmax`, `cross_entropy`, `reshape`, `permute`, `view` 等。可通过查阅 [Jittor API 文档](https://cg.cs.tsinghua.edu.cn/jittor/api/) 获取对应函数的替代实现。

---

## 2. 如何将 `torch.nn.Module` 转换为 Jittor 模型？

**答：** 继承的基类把 `torch.nn.Module` 替换成 `jittor.nn.Module` ，并使用jittor.nn中的函数重写 `__init__` 和 `execute`  `(即forward方法)` 方法，替换掉对应的torch.nn中的函数。例如：

```python
# PyTorch 写法
class Net(torch.nn.Module):
    def __init__(self):
        super().__init__()
        self.linear = torch.nn.Linear(10, 5)
    def forward(self, x):
        return self.linear(x)

# Jittor 写法
class Net(jittor.nn.Module):
    def __init__(self):
        super().__init__()
        self.linear = jittor.nn.Linear(10, 5)
    def execute(self, x):
        return self.linear(x)
```

---

## 3. Jittor 的张量与 PyTorch 的 `tensor` 有什么区别？

**答：**
- PyTorch 使用 `torch.tensor`，Jittor 使用 `jt.array` 。
- Jittor 的基础类型是 `Var`，对应 Torch 里边的 `Tensor`。
- 两者的数据结构类似，但 Jittor 默认支持动态图与静态图切换，所有计算图操作默认记录。
- Jittor 默认使用 `float32` 类型，可通过 `dtype=jt.float64` 等指定精度。
- Jittor 可以使用 `jittor.array(data)` 从np类型转成 `Var` 类型。

---

## 4. 如何进行梯度反向传播和参数优化？

**答：**
Jittor中反向传播和参数优化的方法略有区别，主要是在loss的使用上，示例程序如下所示：

```python
# PyTorch
# 定义优化器
optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)
# 反向传播
optimizer.zero_grad()
loss.backward()
optimizer.step()


# Jittor
# 定义优化器
optimizer = jt.optim.Adam(model.parameters(), lr=0.1, betas=(0.9, 0.999), weight_decay=0.0005)
# 反向传播
optimizer.zero_grad()
optimizer.backward(loss)
optimizer.step()
```
---
## 5. 使用Jittor如何加载数据集？

**答：**
Jittor中使用jittor.dataset.Dataset作为基类定义数据集，并重写__len__(),__init()__,__getitem()__ 等方法，示例代码如下所示：

```python
import jittor.dataset as data
class SentiFeature(data.Dataset):
    def __init__(self, data_path, mode, modal, feature_fps, num_segments, sampling, seed=-1, supervision='point'):
        super().__init__()
    def __len__(self):
        return len(self.vid_list)
    def __getitem__(self, index):
        return data[index]
```
jittor的数据加载器DataLoder位于 jittor.dataset 中 使用 from jittor.dataset import DataLoader导入，使用方式和torch类似。


---

## 6. 使用Jittor如何加载预训练模型或保存模型？

**答：**
jittor中保存和加载预训练模型的方法和torch中类似，jittor中具有同名的函数，只需要把torch替换成jittor即可。
```python
# 保存模型参数
jt.save(model.state_dict(), "model.pth")

# 加载参数
model.load_parameters(jt.load("model.pth"))
```
Jittor 支持加载 `.pth` 格式的预训练模型，也支持使用torch训练的预训练模型参数的加载`(.pth格式)`。

---

## 7. PyTorch 的 `with torch.no_grad()` 在 Jittor 中如何处理？

**答：** Jittor 通过 `jt.no_grad()` 实现类似功能，用于测试阶段关闭梯度：

```python
with jt.no_grad():
    output = model(x)
```

---

## 8. Jittor中如何使用 GPU 运算？是否需要 `.cuda()`？

**答：**
Jittor 自动检测 GPU 并优先使用。Jittor会进行统一内存管理，不需要在手动调用 `.cuda()`,`.cpu()`,`.to(device)` 等函数，如果需要明确设置设备，可以使用：

```python
jt.flags.use_cuda = 1  # 开启 GPU
jt.flags.use_cuda = 0  # 使用 CPU
```

---


## 9. 迁移时遇到 PyTorch 特有的函数怎么办？

**答：**
建议采取以下方案：
- 查找 Jittor 是否有对应 API；
- 手动实现替代逻辑；
- 查阅社区资源或 GitHub 仓库寻找迁移方案；
- 如功能缺失，可通过自定义 `jt.Function` 编写 C++/CUDA 插件。

---


