#  åŸºäºå›½äº§æ·±åº¦æ¡†æ¶Jittorè®¡å›¾çš„è®­ç»ƒä¸éƒ¨ç½²è§£å†³æ–¹æ¡ˆ

<p align="center">
    <br>
    <img src="assets/logo.png"/>
    <br>
<p>
<p align="center">
<a href="">é¡¹ç›®ä¸»é¡µ</a>
<br>
        ä¸­æ–‡&nbsp ï½œ &nbsp<a href="README.md">English</a>&nbsp
</p>
<p align="center">
<img src="https://img.shields.io/badge/python-3.8-5be.svg">
<img src="https://img.shields.io/badge/jittor-1.3.9-orange.svg">
<a href="https://github.com/zhongqihebut/Affective_Computing/blob/master/LICENSE"><img src="https://img.shields.io/github/license/zhongqihebut/Affective_Computing"></a>
<a href="https://github.com/zhongqihebut/Affective_Computing/pulls"><img src="https://img.shields.io/badge/PR-welcome-55EB99.svg"></a>
</p>

<p align="center">
        <a href="./docs/en/papers.md">ç›¸å…³è®ºæ–‡</a> &nbsp ï½œ <a href="./docs/en">English Documentation</a> &nbsp ï½œ &nbsp <a href="./docs/cn">ä¸­æ–‡æ–‡æ¡£</a> &nbsp
</p>

## ğŸ“– ç›®å½•
- [ç®€ä»‹](#-ç®€ä»‹)
- [æ–°é—»](#-æ–°é—»)
- [å®‰è£…](#%EF%B8%8F-å®‰è£…)
- [ä½¿ç”¨](#-ä½¿ç”¨)
- [License](#-License)
- [å¼•ç”¨](#-å¼•ç”¨)

  



## ğŸ“ ç®€ä»‹

***æƒ…æ™ºå…¼å¤‡*** æ˜¯æ–°ä¸€ä»£äººå·¥æ™ºèƒ½çš„é‡è¦å‘å±•æ–¹å‘ï¼Œæ˜¯è¿ˆå‘é€šç”¨äººå·¥æ™ºèƒ½çš„å…³é”®ä¸€æ­¥ã€‚åœ¨äººæœºäº¤äº’åœºæ™¯ä¸­ï¼Œå…·å¤‡æƒ…æ™ºçš„æ•°å­—äººä¸æœºå™¨äººéœ€è¦ç²¾å‡†è§£è¯‘å¤šæ¨¡æ€äº¤äº’ä¿¡æ¯ï¼Œæ·±åº¦æŒ–æ˜äººç±»å†…åœ¨æƒ…æ„ŸçŠ¶æ€ï¼Œä»è€Œå®ç°æ›´å…·çœŸå®æ„Ÿä¸è‡ªç„¶æ€§çš„äººæœºå¯¹è¯ã€‚ç„¶è€Œï¼Œé¢å¯¹å¤šæ¨¡æ€æƒ…æ„Ÿæ•°æ®è¯­ä¹‰çš„é«˜åº¦å¤æ‚æ€§ï¼Œå¦‚ä½•æœ‰æ•ˆå»ºæ¨¡è·¨æ¨¡æ€å…³è”å…³ç³»ä»æ˜¯é¢†åŸŸå†…äºŸå¾…çªç ´çš„æ ¸å¿ƒæŒ‘æˆ˜ã€‚

JACKï¼ˆåŸºäºJittorçš„æƒ…æ„Ÿè®¡ç®—æ¨¡å‹è®­ç»ƒä¸éƒ¨ç½²æ¡†æ¶ï¼‰æ˜¯ç”±å—å¼€å¤§å­¦è®¡ç®—æœºè§†è§‰å›¢é˜Ÿæä¾›çš„å®˜æ–¹æ¡†æ¶ï¼ŒåŸºäºå›½äº§åŒ–é«˜æ€§èƒ½æ·±åº¦å­¦ä¹ æ¡†æ¶è®¡å›¾ï¼ˆJittorï¼‰è¿›è¡Œæƒ…æ„Ÿè®¡ç®—æ–¹æ³•çš„è®­ç»ƒä¸éƒ¨ç½²ã€‚ç›®å‰ï¼Œè¯¥æ¡†æ¶æ”¯æŒå…ˆè¿›çš„è§†é¢‘æƒ…æ„Ÿåˆ†ææ–¹æ³•ä»¥åŠæ­¥æ€è§†é¢‘æƒ…æ„Ÿåˆ†ææ–¹æ³•ã€‚åŸºäºJittorå›½äº§æ¡†æ¶ï¼Œæƒ…æ„Ÿè®¡ç®—æ–¹æ³•çš„éƒ¨ç½²é€Ÿåº¦ç›¸æ¯”PyTorchå¯æå‡1.1è‡³1.6å€ï¼Œä»è€Œæ”¯æŒä¸‹æ¸¸åº”ç”¨å¦‚æ¸¸å®¢æƒ…æ„Ÿæ£€æµ‹ã€å¯¹è¯åˆ†æã€èˆ†æƒ…ç›‘æ§ç­‰ã€‚



è¯¥é¡¹ç›®ç›®å‰æ”¯æŒåŸºäºJittoræ·±åº¦å­¦ä¹ æ¡†æ¶çš„æƒ…æ„Ÿè®¡ç®—é¢†åŸŸä¸­çš„å››é¡¹å·¥ä½œï¼š

| **å·¥ä½œ**| **è®­ç»ƒ** | **æµ‹è¯•** |
|-----------------------------------------------------------------------------------------------------------|-----------|----------|
| [[CVPR'23] CTEN](https://openaccess.thecvf.com/content/CVPR2023/papers/Zhang_Weakly_Supervised_Video_Emotion_Detection_and_Prediction_via_Cross-Modal_Temporal_CVPR_2023_paper.pdf) | [[è®­ç»ƒè„šæœ¬]](./docs/cn/è®­ç»ƒ.md#-CTEN) | [[æµ‹è¯•è„šæœ¬]](./docs/cn/æµ‹è¯•.md#-CTEN) |
| [[MM'22] TSL-Net](https://github.com/nku-zhichengzhang/TSL300/blob/main/assests/acm22_zzc_videosenti_official.pdf) | [[è®­ç»ƒè„šæœ¬]](./docs/cn/è®­ç»ƒ.md#-TSL-Net) | [[æµ‹è¯•è„šæœ¬]](./docs/cn/æµ‹è¯•.md#-TSL-Net) |
| [[AAAI'20] VAANet](https://arxiv.org/abs/2003.00832)                                                              | [[è®­ç»ƒè„šæœ¬]](./docs/cn/è®­ç»ƒ.md#-TSL-Net) | [[æµ‹è¯•è„šæœ¬]](./docs/cn/æµ‹è¯•.md#-TSL-Net) |
| [[TAC'24] Gait](https://ieeexplore.ieee.org/document/10433680)                                                   | [[è®­ç»ƒè„šæœ¬]](./docs/cn/è®­ç»ƒ.md#-Gait) | [[æµ‹è¯•è„šæœ¬]](./docs/cn/æµ‹è¯•.md#-Gait) |


åœ¨æƒ…æ„Ÿè®¡ç®—é¢†åŸŸï¼ŒJittoré«˜æ€§èƒ½æ·±åº¦å­¦ä¹ æ¡†æ¶ä¸PyTorchæ¡†æ¶çš„æ€§èƒ½å¯¹æ¯”ï¼š


| TSL-NetæŒ‡æ ‡                        | PyTorch  | Jittor  |
|-----------------------------|----------|---------|
| å¹³å‡å‰å‘æ—¶é—´ï¼ˆè®­ç»ƒï¼‰         | 0.324s   | 0.068s  |
| å¹³å‡å†…å­˜ä½¿ç”¨é‡ï¼ˆè®­ç»ƒï¼‰       | 11319MB  | 16132MB |
| å•æ¬¡è¿­ä»£æ—¶é—´ï¼ˆè®­ç»ƒï¼‰         | 1.162s   | 0.981s  |
| å¹³å‡_mAP[0.1:0.3]ï¼ˆæµ‹è¯•ï¼‰    | 0.1985   | 0.1949  |
| å¹³å‡_pAP[0.1:0.3]ï¼ˆæµ‹è¯•ï¼‰    | 0.2106   | 0.2095  |
| å¹³å‡_nAP[0.1:0.3]ï¼ˆæµ‹è¯•ï¼‰    | 0.1865   | 0.1803  |
| F2@AVGï¼ˆæµ‹è¯•ï¼‰               | 0.3369   | 0.3577  |



## ğŸ‰ æ–°é—»
- ğŸ 2025.07.16: é¡¹ç›®åˆå§‹åŒ–ã€‚æœ¬é¡¹ç›®æ”¯æŒå››é¡¹è§†é¢‘æƒ…æ„Ÿåˆ†æä»»åŠ¡ï¼ŒåŒ…æ‹¬ [CTEN](https://openaccess.thecvf.com/content/CVPR2023/papers/Zhang_Weakly_Supervised_Video_Emotion_Detection_and_Prediction_via_Cross-Modal_Temporal_CVPR_2023_paper.pdf), [TSL_Net](https://github.com/nku-zhichengzhang/TSL300/blob/main/assests/acm22_zzc_videosenti_official.pdf), [VAANet](https://arxiv.org/abs/2003.00832), å’Œ [Gait](https://ieeexplore.ieee.org/document/10433680)ã€‚æ‰€æœ‰æ–¹æ³•å‡æä¾›äº†è®­ç»ƒå’Œæµ‹è¯•è„šæœ¬ã€‚


## ğŸ› ï¸ å®‰è£…
ä½¿ç”¨ pip å®‰è£…ï¼š

```shell
pip install -r requirements.txt
```

ä¾èµ–çš„ç¯å¢ƒ:

| åŒ…            | èŒƒå›´         | æ¨è        | æ³¨æ„                                       |
| ------------ |--------------| ---------- | ----------------------------------------- |
| python       | >=3.8        | 3.8        |                                           |
| cuda         |              | cuda11.3   | å¦‚æœä½¿ç”¨ CPUã€NPU æˆ– MPSï¼Œåˆ™æ— éœ€å®‰è£…          |
| jittor       |              | 1.3.9.14   |                                           |
| transformers | >=4.30       | 4.31.0     |                                           |

æœ‰å…³æ›´å¤šå¯é€‰ä¾èµ–é¡¹ï¼Œè¯·å‚è€ƒ [è¿™é‡Œ](./docs/en/env.md)ã€‚




## âœ¨ ä½¿ç”¨
ä»¥ä¸‹æ˜¯ä½¿ç”¨ JACK è¿›è¡Œè®­ç»ƒå’Œéƒ¨ç½²çš„ç®€è¦ç¤ºä¾‹ï¼š

- å¦‚æœæ‚¨å¸Œæœ›ä½¿ç”¨å…¶ä»–æ¨¡å‹æˆ–æ•°æ®é›†ï¼ˆåŒ…æ‹¬å¤šæ¨¡æ€æ¨¡å‹å’Œæ•°æ®é›†ï¼‰ï¼Œåªéœ€ä¿®æ”¹ `--model` æ¥æŒ‡å®šå¯¹åº”æ¨¡å‹çš„åå­—ï¼Œå¹¶ä¿®æ”¹ `--dataset` æ¥æŒ‡å®šå¯¹åº”æ•°æ®é›†çš„è·¯å¾„ã€‚

|  å®ç”¨é“¾æ¥ |
| ------ |
|   [ğŸ”¥æ”¯æŒçš„æ–¹æ³•](./docs/en/papers.md)   |
|   [è®­ç»ƒ](./docs/en/train.md)   |
|   [æµ‹è¯•](./docs/en/test.md) |
|   [æ•°æ®é›†](./docs/en/dataset.md)   |
|   [Torchè½¬Jittorå¸¸è§é—®é¢˜](./docs/en/FAQ.md)   |



## ğŸ› License

æœ¬æ¡†æ¶ä½¿ç”¨[Apache License (Version 2.0)](https://github.com/modelscope/modelscope/blob/master/LICENSE)è¿›è¡Œè®¸å¯ã€‚æ¨¡å‹å’Œæ•°æ®é›†è¯·æŸ¥çœ‹åŸèµ„æºé¡µé¢å¹¶éµå®ˆå¯¹åº”Licenseã€‚


## ğŸ“ å¼•ç”¨

```bibtex
@inproceedings{zhang2025moda,
  author = {Zhang, Zhicheng and Xia, Wuyou and Zhao, Chenxi and Yan, Zhou and Liu, Xiaoqiang and Zhu, Yongjie and Qin, Wenyu and Wan, Pengfei and Zhang, Di and Yang, Jufeng},
  title = {MODA: MOdular Duplex Attention for Multimodal Perception, Cognition, and Emotion Understanding},
  booktitle = {Proceedings of the 42nd International Conference on Machine Learning (ICML)},
  year = {2025},
}
```
```bibtex
@inproceedings{zhang2024masked,
  author = {Zhang, Zhicheng and Zhao, Pancheng and Park, Eunil and Yang, Jufeng},
  title = {MART: Masked Affective RepresenTation Learning via Masked Temporal Distribution Distillation},
  booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
  year = {2024},
}
```

```bibtex
@inproceedings{Zhang_2023_CVPR,
  author = {Zhang, Zhicheng and Wang, Lijuan and Yang, Jufeng},
  title = {Weakly Supervised Video Emotion Detection and Prediction via Cross-Modal Temporal Erasing Network},
  booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
  year = {2023},
}
```
```bibtex
@inproceedings{10.1145/3503161.3548007,
  author = {Zhang, Zhicheng and Yang, Jufeng},
  title = {Temporal Sentiment Localization: Listen and Look in Untrimmed Videos},
  year = {2022},
  booktitle = {Proceedings of the 30th ACM International Conference on Multimedia},
}
```
```bibtex
@inproceedings{zhao2020end,
  title={An end-to-end visual-audio attention network for emotion recognition in user-generated videos},
  author={Zhao, Sicheng and Ma, Yunsheng and Gu, Yang and Yang, Jufeng and Xing, Tengfei and Xu, Pengfei and Hu, Runbo and Chai, Hua and Keutzer, Kurt},
  booktitle={Proceedings of the AAAI conference on artificial intelligence},
  year={2020}
}
```
```bibtex
@article{zhai2024looking,
  title={Looking into gait for perceiving emotions via bilateral posture and movement graph convolutional networks},
  author={Zhai, Yingjie and Jia, Guoli and Lai, Yu-Kun and Zhang, Jing and Yang, Jufeng and Tao, Dacheng},
  journal={IEEE Transactions on Affective Computing},
  volume={15},
  number={3},
  pages={1634--1648},
  year={2024},
}
```