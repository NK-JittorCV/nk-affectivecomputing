#  åŸºäºå›½äº§æ·±åº¦æ¡†æ¶Jittorè®¡å›¾çš„è®­ç»ƒä¸éƒ¨ç½²è§£å†³æ–¹æ¡ˆ

<p align="center">
    <br>
    <img src="assets/logo.png"/>
    <br>
<p>
<p align="center">
<a href="">é¡¹ç›®ä¸»é¡µ</a>
<br>
        ä¸­æ–‡&nbsp ï½œ &nbsp<a href="README.md">English</a>&nbsp
</p>
<p align="center">
<img src="https://img.shields.io/badge/python-3.8-5be.svg">
<img src="https://img.shields.io/badge/jittor-1.3.9-orange.svg">
<a href="https://github.com/zhongqihebut/Affective_Computing/blob/master/LICENSE"><img src="https://img.shields.io/github/license/zhongqihebut/Affective_Computing"></a>
<a href="https://github.com/zhongqihebut/Affective_Computing/pulls"><img src="https://img.shields.io/badge/PR-welcome-55EB99.svg"></a>
</p>

<p align="center">
        <a href="./docs/en/papers.md">ç›¸å…³è®ºæ–‡</a> &nbsp ï½œ <a href="./docs/en">English Documentation</a> &nbsp ï½œ &nbsp <a href="./docs/cn">ä¸­æ–‡æ–‡æ¡£</a> &nbsp
</p>

## ğŸ“– ç›®å½•
- [åŸºäºå›½äº§æ·±åº¦æ¡†æ¶Jittorè®¡å›¾çš„è®­ç»ƒä¸éƒ¨ç½²è§£å†³æ–¹æ¡ˆ](#åŸºäºå›½äº§æ·±åº¦æ¡†æ¶jittorè®¡å›¾çš„è®­ç»ƒä¸éƒ¨ç½²è§£å†³æ–¹æ¡ˆ)
  - [ğŸ“– ç›®å½•](#-ç›®å½•)
  - [ğŸ“ ç®€ä»‹](#-ç®€ä»‹)
  - [ğŸ‰ æ–°é—»](#-æ–°é—»)
  - [ğŸ› ï¸ å®‰è£…](#ï¸-å®‰è£…)
  - [âœ¨ ä½¿ç”¨](#-ä½¿ç”¨)
    - [âœ¨ æ·±å…¥ä½¿ç”¨JACKå¹¶å®šåˆ¶åŒ–å‚æ•°](#-æ·±å…¥ä½¿ç”¨jackå¹¶å®šåˆ¶åŒ–å‚æ•°)
      - [è®­ç»ƒ](#è®­ç»ƒ)
      - [æµ‹è¯•](#æµ‹è¯•)
      - [CTENå‚æ•°è¯´æ˜](#ctenå‚æ•°è¯´æ˜)
  - [ğŸ› License](#-license)
  - [ğŸ“ å¼•ç”¨](#-å¼•ç”¨)

  



## ğŸ“ ç®€ä»‹

***æƒ…æ™ºå…¼å¤‡*** æ˜¯æ–°ä¸€ä»£äººå·¥æ™ºèƒ½çš„é‡è¦å‘å±•æ–¹å‘ï¼Œæ˜¯è¿ˆå‘é€šç”¨äººå·¥æ™ºèƒ½çš„å…³é”®ä¸€æ­¥ã€‚åœ¨äººæœºäº¤äº’åœºæ™¯ä¸­ï¼Œå…·å¤‡æƒ…æ™ºçš„æ•°å­—äººä¸æœºå™¨äººéœ€è¦ç²¾å‡†è§£è¯‘å¤šæ¨¡æ€äº¤äº’ä¿¡æ¯ï¼Œæ·±åº¦æŒ–æ˜äººç±»å†…åœ¨æƒ…æ„ŸçŠ¶æ€ï¼Œä»è€Œå®ç°æ›´å…·çœŸå®æ„Ÿä¸è‡ªç„¶æ€§çš„äººæœºå¯¹è¯ã€‚ç„¶è€Œï¼Œé¢å¯¹å¤šæ¨¡æ€æƒ…æ„Ÿæ•°æ®è¯­ä¹‰çš„é«˜åº¦å¤æ‚æ€§ï¼Œå¦‚ä½•æœ‰æ•ˆå»ºæ¨¡è·¨æ¨¡æ€å…³è”å…³ç³»ä»æ˜¯é¢†åŸŸå†…äºŸå¾…çªç ´çš„æ ¸å¿ƒæŒ‘æˆ˜ã€‚

JACKï¼ˆåŸºäºJittorçš„æƒ…æ„Ÿè®¡ç®—æ¨¡å‹è®­ç»ƒä¸éƒ¨ç½²æ¡†æ¶ï¼‰æ˜¯ç”±å—å¼€å¤§å­¦è®¡ç®—æœºè§†è§‰å›¢é˜Ÿæä¾›çš„å®˜æ–¹æ¡†æ¶ï¼ŒåŸºäºå›½äº§åŒ–é«˜æ€§èƒ½æ·±åº¦å­¦ä¹ æ¡†æ¶è®¡å›¾ï¼ˆJittorï¼‰è¿›è¡Œæƒ…æ„Ÿè®¡ç®—æ–¹æ³•çš„è®­ç»ƒä¸éƒ¨ç½²ã€‚ç›®å‰ï¼ŒJACKæ¡†æ¶æ”¯æŒå…ˆè¿›çš„è§†é¢‘æƒ…æ„Ÿåˆ†ææ–¹æ³•ä»¥åŠæ­¥æ€è§†é¢‘æƒ…æ„Ÿåˆ†ææ–¹æ³•ã€‚åŸºäºJittorå›½äº§æ¡†æ¶ï¼Œæƒ…æ„Ÿè®¡ç®—æ–¹æ³•çš„éƒ¨ç½²é€Ÿåº¦ç›¸æ¯”PyTorchå¯æå‡1.1è‡³1.6å€ï¼Œä»è€Œæ”¯æŒä¸‹æ¸¸åº”ç”¨å¦‚æ¸¸å®¢æƒ…æ„Ÿæ£€æµ‹ã€å¯¹è¯åˆ†æã€èˆ†æƒ…ç›‘æ§ç­‰ã€‚

Jittorå›½äº§æ·±åº¦å­¦ä¹ æ¡†æ¶èƒ½å¤Ÿæ— ç¼å…¼å®¹ä¸»æµçš„PyTorchæ¡†æ¶ã€‚ä»¥[`TSL-Net`](https://zzcheng.top/assets/pdf/2022_ACMMM_TSL300.pdf)ç½‘ç»œæ¶æ„ä¸ºä¾‹ï¼Œåœ¨å…¼å®¹ä¿®æ”¹åˆ°JACKä¸­æ—¶ï¼Œæ¨¡å‹ä»£ç ä»…éœ€è¦ä¿®æ”¹10ä½™å¤„å³å¯å®Œæˆè½¬æ¢ï¼Œå¤§å¹…é™ä½äº†è¿ç§»æˆæœ¬ã€‚åŒæ—¶ï¼Œæˆ‘ä»¬è¿˜åœ¨[`Torchè½¬Jittorå¸¸è§é—®é¢˜.md`](./docs/cn/Torchè½¬Jittorå¸¸è§é—®é¢˜.md)ä¸­æä¾›äº†è¯¦ç»†çš„è½¬æ¢ç»éªŒä¸æŒ‡å¯¼ï¼Œå¸®åŠ©å¼€å‘è€…å¿«é€Ÿä¸Šæ‰‹ã€‚æˆ‘ä»¬è¯šé‚€æ›´å¤šç ”ç©¶è€…å‚ä¸ï¼Œå…±åŒæ¨è¿›æƒ…æ„Ÿè®¡ç®—é¢†åŸŸçš„å›½äº§åŒ–è¿›ç¨‹ï¼è®©æˆ‘ä»¬æºæ‰‹æ‰“é€ æ›´å¼ºå¤§çš„å›½äº§AIç”Ÿæ€ï¼

åœ¨Jittoræ·±åº¦å­¦ä¹ æ¡†æ¶åŠ©åŠ›ä¸‹ï¼Œè¯¥é¡¹ç›®å·²æ”¯æŒæƒ…æ„Ÿè®¡ç®—é¢†åŸŸä¸­çš„æœ€æ–°å·¥ä½œï¼š

| **å·¥ä½œ**| **è®­ç»ƒ** | **æµ‹è¯•** |
|-----------------------------------------------------------------------------------------------------------|-----------|----------|
| [`[CVPR'23] CTEN`](https://openaccess.thecvf.com/content/CVPR2023/papers/Zhang_Weakly_Supervised_Video_Emotion_Detection_and_Prediction_via_Cross-Modal_Temporal_CVPR_2023_paper.pdf) | [`[è®­ç»ƒè„šæœ¬]`](./docs/cn/è®­ç»ƒ.md#-CTEN) | [`[æµ‹è¯•è„šæœ¬]`](./docs/cn/æµ‹è¯•.md#-CTEN) |
| [`[MM'22] TSL-Net`](https://zzcheng.top/assets/pdf/2022_ACMMM_TSL300.pdf) | [`[è®­ç»ƒè„šæœ¬]`](./docs/cn/è®­ç»ƒ.md#-TSL-Net) | [`[æµ‹è¯•è„šæœ¬]`](./docs/cn/æµ‹è¯•.md#-TSL-Net) |
| [`[AAAI'20] VAANet`](https://arxiv.org/abs/2003.00832) | [`[è®­ç»ƒè„šæœ¬]`](./docs/cn/è®­ç»ƒ.md#-TSL-Net) | [`[æµ‹è¯•è„šæœ¬]`](./docs/cn/æµ‹è¯•.md#-TSL-Net) |
| [`[TAC'24] Gait`](https://ieeexplore.ieee.org/document/10433680) | [`[è®­ç»ƒè„šæœ¬]`](./docs/cn/è®­ç»ƒ.md#-Gait) | [`[æµ‹è¯•è„šæœ¬]`](./docs/cn/æµ‹è¯•.md#-Gait) |


åœ¨æƒ…æ„Ÿè®¡ç®—æ–¹æ³•[`TSL-Net`](https://zzcheng.top/assets/pdf/2022_ACMMM_TSL300.pdf)ï¼ŒJittoré«˜æ€§èƒ½æ·±åº¦å­¦ä¹ æ¡†æ¶æ¯”PyTorchæ¡†æ¶æ¨ç†é€Ÿåº¦æ›´å¿«ã€è®­ç»ƒæ—¶é•¿æ›´çŸ­ã€è¾“å‡ºç»“æœä¸€è‡´ï¼š


| TSL-NetæŒ‡æ ‡                        | PyTorch  | Jittor  |
|-----------------------------|----------|---------|
| å¹³å‡å‰å‘æ—¶é—´ï¼ˆè®­ç»ƒï¼‰         | 0.324s   | 0.068s  |
| å¹³å‡å†…å­˜ä½¿ç”¨é‡ï¼ˆè®­ç»ƒï¼‰       | 11319MB  | 16132MB |
| å•æ¬¡è¿­ä»£æ—¶é—´ï¼ˆè®­ç»ƒï¼‰         | 1.162s   | 0.981s  |
| å¹³å‡_mAP[0.1:0.3]ï¼ˆæµ‹è¯•ï¼‰    | 0.1985   | 0.1949  |
| å¹³å‡_pAP[0.1:0.3]ï¼ˆæµ‹è¯•ï¼‰    | 0.2106   | 0.2095  |
| å¹³å‡_nAP[0.1:0.3]ï¼ˆæµ‹è¯•ï¼‰    | 0.1865   | 0.1803  |
| F2@AVGï¼ˆæµ‹è¯•ï¼‰               | 0.3369   | 0.3577  |



## ğŸ‰ æ–°é—»
- ğŸ 2025.07.16: é¡¹ç›®åˆå§‹åŒ–ã€‚æœ¬é¡¹ç›®æ”¯æŒå››é¡¹è§†é¢‘æƒ…æ„Ÿåˆ†æä»»åŠ¡ï¼ŒåŒ…æ‹¬ [CTEN](https://openaccess.thecvf.com/content/CVPR2023/papers/Zhang_Weakly_Supervised_Video_Emotion_Detection_and_Prediction_via_Cross-Modal_Temporal_CVPR_2023_paper.pdf), [TSL_Net](https://zzcheng.top/assets/pdf/2022_ACMMM_TSL300.pdf), [VAANet](https://arxiv.org/abs/2003.00832), å’Œ [Gait](https://ieeexplore.ieee.org/document/10433680)ã€‚æ‰€æœ‰æ–¹æ³•å‡æä¾›äº†è®­ç»ƒå’Œæµ‹è¯•è„šæœ¬ã€‚


## ğŸ› ï¸ å®‰è£…
ä½¿ç”¨ pip å®‰è£…ï¼š

```shell
pip install -r requirements.txt
```

ä¾èµ–çš„ç¯å¢ƒ:

| åŒ…            | èŒƒå›´         | æ¨è        | æ³¨æ„                                       |
| ------------ |--------------| ---------- | ----------------------------------------- |
| python       | >=3.8        | 3.8        |                                           |
| cuda         |              | cuda11.3   | å¦‚æœä½¿ç”¨ CPUã€NPU æˆ– MPSï¼Œåˆ™æ— éœ€å®‰è£…          |
| jittor       |              | 1.3.9.14   |                                           |
| transformers | >=4.30       | 4.31.0     |                                           |

æœ‰å…³æ›´å¤šå¯é€‰ä¾èµ–é¡¹ï¼Œè¯·å‚è€ƒ [`ç¯å¢ƒé…ç½®.md`](./docs/cn/ç¯å¢ƒé…ç½®.md)ã€‚




## âœ¨ ä½¿ç”¨
ä»¥ä¸‹æ˜¯ä½¿ç”¨ JACK è¿›è¡Œè®­ç»ƒå’Œéƒ¨ç½²çš„ç®€è¦ç¤ºä¾‹ï¼š

è®­ç»ƒ
```
bash script/run.sh CTEN main
```

æµ‹è¯•
```
bash script/run.sh CTEN test
```

- å¦‚æœæ‚¨å¸Œæœ›ä½¿ç”¨å…¶ä»–æƒ…æ„Ÿè®¡ç®—æ¨¡å‹ï¼Œåªéœ€ä¿®æ”¹ç¬¬ä¸€ä¸ªå‚æ•°æ¥æŒ‡å®šå¯¹åº”æ¨¡å‹çš„åå­—ï¼Œå¹¶ä¿®æ”¹ç¬¬äºŒä¸ªå‚æ•°æ¥æŒ‡å®šå¯¹åº”åŠŸèƒ½ï¼ˆä¾‹å¦‚è®­ç»ƒ/æµ‹è¯•ï¼‰ã€‚

### âœ¨ æ·±å…¥ä½¿ç”¨JACKå¹¶å®šåˆ¶åŒ–å‚æ•°

åœ¨è¿›ä¸€æ­¥æ¢ç´¢ä½¿ç”¨å‰ï¼Œå»ºè®®æå‰é˜…è¯»æˆ‘ä»¬çš„æ–‡æ¡£ä»¥ä¾¿æ‚¨å¯¹æœ¬é¡¹ç›®æ”¯æŒçš„åŠŸèƒ½æœ‰æ›´å¥½çš„ç†è§£ã€‚

|  **å®ç”¨é“¾æ¥** |
| ------ |
|   [ğŸ”¥æ”¯æŒçš„æ–¹æ³•](./docs/cn/æ”¯æŒå·¥ä½œ.md)   |
|   [è®­ç»ƒ](./docs/cn/è®­ç»ƒ.md)   |
|   [æµ‹è¯•](./docs/cn/æµ‹è¯•.md) |
|   [æ•°æ®é›†](./docs/cn/æ•°æ®é›†å‡†å¤‡.md)   |
|   [Torchè½¬Jittorå¸¸è§é—®é¢˜](./docs/cn/Torchè½¬Jittorå¸¸è§é—®é¢˜.md)   |

ä»¥ [CTEN](https://openaccess.thecvf.com/content/CVPR2023/papers/Zhang_Weakly_Supervised_Video_Emotion_Detection_and_Prediction_via_Cross-Modal_Temporal_CVPR_2023_paper.pdf) ä¸ºä¾‹ï¼ŒJACK æä¾›äº†ä»è®­ç»ƒåˆ°éƒ¨ç½²çš„å®Œæ•´è§£å†³æ–¹æ¡ˆã€‚

---

#### è®­ç»ƒ
```
bash script/run.sh CTEN main \
--dataset ve8 \
--resnet101_pretrained your_path \
--video_path your_path your_path \
--annotation_path src/CTEN/data/ve8_04.json \
--audio_path your_path \
--result_path your_path \
--batch_size 4 \
--n_epochs 100 \
--sample_size 112 \
--fps 30 \
--snippet_duration 16 \
--audio_embed_size 2048 \
--audio_n_segments 16 \
--audio_time 100
```

åœ¨è¿è¡Œè„šæœ¬ä¹‹å‰ï¼Œè¯·ç¡®ä¿å°†è§†é¢‘è·¯å¾„ã€éŸ³é¢‘è·¯å¾„ã€é¢„è®­ç»ƒæ¨¡å‹è·¯å¾„ä»¥åŠç»“æœè¾“å‡ºè·¯å¾„ä¸­çš„å ä½ç¬¦æ›¿æ¢ä¸ºå®é™…çš„æœ¬åœ°è·¯å¾„ã€‚é…ç½®å®Œæˆåï¼Œæ‚¨å¯ä»¥åœ¨ VE8 æ•°æ®é›†ä¸Šè®­ç»ƒæ¨¡å‹ä»¥è·å–æœ€ç»ˆçš„è®­ç»ƒæƒé‡æ–‡ä»¶ã€‚æœ‰å…³æ¯ä¸ªå‚æ•°çš„è¯¦ç»†è¯´æ˜ï¼Œè¯·å‚è€ƒ [`Train`](./docs/en/train.md) æ–‡æ¡£ã€‚

---

#### æµ‹è¯•
```
bash script/run.sh CTEN test \
--dataset ve8 \
--video_path your_path your_path \
--audio_path your_path \
--result_path your_path
```
âš ï¸ æ³¨æ„ï¼šå¦‚æœåœ¨è®­ç»ƒæœŸé—´ä½¿ç”¨äº†éšæœºæ•°æ®å¢å¼ºï¼Œæ¯æ¬¡è¿è¡Œè„šæœ¬æ—¶ï¼Œé¢„æµ‹ç»“æœå¯èƒ½ä¼šç•¥æœ‰ä¸åŒã€‚æ›´å¤šä¿¡æ¯è¯·å‚é˜… [`å¸¸è§é—®é¢˜.md`](./docs/cn/Torchè½¬Jittorå¸¸è§é—®é¢˜.md)ã€‚

å¦‚æœæ‚¨çš„ç›®æ ‡ä»…æ˜¯è·å–æ•°æ®é›†ä¸­æ¯ä¸ªè§†é¢‘ç‰‡æ®µçš„æƒ…æ„Ÿåˆ†ç±»ç»“æœï¼Œè¯·æ‰“å¼€ [`test.py`](./src/CTEN/test.py#l80) æ–‡ä»¶å¹¶ä¿®æ”¹ç¬¬ 80 è¡Œï¼Œå°†å ä½ç¬¦å­—ç¬¦ä¸²æ›¿æ¢ä¸ºè®­ç»ƒæƒé‡æ–‡ä»¶çš„è·¯å¾„ã€‚ç„¶åï¼Œä»å‘½ä»¤è¡Œæ‰§è¡Œè„šæœ¬ä»¥ç”Ÿæˆåˆ†ç±»ç»“æœã€‚

#### CTENå‚æ•°è¯´æ˜
- datasetï¼šæ˜¯æŒ‡å®šä½¿ç”¨çš„æ•°æ®é›†åç§°ï¼Œå¯ä»¥å‚è€ƒ[`æ•°æ®é›†.md`](./docs/cn/æ•°æ®é›†å‡†å¤‡.md)ç›¸å…³è¯´æ˜ã€‚
- resnet101_pretrainedï¼šé¢„è®­ç»ƒå›¾åƒæ¨¡å‹æƒé‡è·¯å¾„ï¼ˆå¦‚ ResNet-101ï¼‰ï¼›ç”¨äºè§†é¢‘å¸§ç‰¹å¾æå–ã€‚
- result_pathï¼šæ¨ç†æˆ–è®­ç»ƒç»“æœä¿å­˜è·¯å¾„ã€‚
- video_pathï¼šè§†é¢‘å¸§åºåˆ—æˆ–è§†é¢‘æ–‡ä»¶çš„è·¯å¾„ã€‚
- audio_pathï¼šå¯¹åº”çš„è§†é¢‘éŸ³é¢‘æ–‡ä»¶ï¼ˆå¦‚.mp3ï¼‰æ‰€åœ¨è·¯å¾„ã€‚
- annotation_pathï¼šæ ‡æ³¨æ–‡ä»¶è·¯å¾„ï¼Œæ ‡æ³¨æ–‡ä»¶ç”¨äºè®­ç»ƒè¿˜æ˜¯æµ‹è¯•ä»¥åŠå¯¹åº”çš„æƒ…æ„Ÿåˆ†ç±»ã€‚
- batch_sizeï¼šæ‰¹å¤§å°ï¼šæ¯æ¬¡é€å…¥æ¨¡å‹çš„æ ·æœ¬æ•°é‡ã€‚
- n_epochsï¼šæ€»è®­ç»ƒè½®æ•°ã€‚
- sample_sizeï¼šè§†é¢‘å¸§å›¾åƒçš„è¾“å…¥å°ºå¯¸ï¼ˆå®½é«˜ï¼‰ã€‚
- fpsï¼šè§†é¢‘å¸§ç‡ã€‚
- snippet_durationï¼šæ¯ä¸ª clip çš„æŒç»­å¸§æ•°ã€‚
- audio_embed_sizeï¼š éŸ³é¢‘ç‰¹å¾ç»´åº¦å¤§å°ã€‚
- audio_n_segmentsï¼šå°†æ•´ä¸ªéŸ³é¢‘åˆ’åˆ†ä¸º 16 ä¸ªæ®µï¼Œæ¯æ®µæå–ä¸€ä¸ª embeddingï¼›å¯¹åº”å¸§çš„å¯¹é½ã€‚
- audio_timeï¼šæ¯æ®µéŸ³é¢‘é‡‡æ ·çš„æ—¶é•¿ã€‚

## ğŸ› License

æœ¬æ¡†æ¶ä½¿ç”¨[Apache License (Version 2.0)](https://github.com/modelscope/modelscope/blob/master/LICENSE)è¿›è¡Œè®¸å¯ã€‚æ¨¡å‹å’Œæ•°æ®é›†è¯·æŸ¥çœ‹åŸèµ„æºé¡µé¢å¹¶éµå®ˆå¯¹åº”Licenseã€‚


## ğŸ“ å¼•ç”¨

```bibtex
@inproceedings{zhang2025moda,
  author = {Zhang, Zhicheng and Xia, Wuyou and Zhao, Chenxi and Yan, Zhou and Liu, Xiaoqiang and Zhu, Yongjie and Qin, Wenyu and Wan, Pengfei and Zhang, Di and Yang, Jufeng},
  title = {MODA: MOdular Duplex Attention for Multimodal Perception, Cognition, and Emotion Understanding},
  booktitle = {Proceedings of the 42nd International Conference on Machine Learning (ICML)},
  year = {2025},
}
```
```bibtex
@inproceedings{zhang2024masked,
  author = {Zhang, Zhicheng and Zhao, Pancheng and Park, Eunil and Yang, Jufeng},
  title = {MART: Masked Affective RepresenTation Learning via Masked Temporal Distribution Distillation},
  booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
  year = {2024},
}
```

```bibtex
@inproceedings{Zhang_2023_CVPR,
  author = {Zhang, Zhicheng and Wang, Lijuan and Yang, Jufeng},
  title = {Weakly Supervised Video Emotion Detection and Prediction via Cross-Modal Temporal Erasing Network},
  booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
  year = {2023},
}
```
```bibtex
@inproceedings{10.1145/3503161.3548007,
  author = {Zhang, Zhicheng and Yang, Jufeng},
  title = {Temporal Sentiment Localization: Listen and Look in Untrimmed Videos},
  year = {2022},
  booktitle = {Proceedings of the 30th ACM International Conference on Multimedia},
}
```
```bibtex
@inproceedings{zhao2020end,
  title={An end-to-end visual-audio attention network for emotion recognition in user-generated videos},
  author={Zhao, Sicheng and Ma, Yunsheng and Gu, Yang and Yang, Jufeng and Xing, Tengfei and Xu, Pengfei and Hu, Runbo and Chai, Hua and Keutzer, Kurt},
  booktitle={Proceedings of the AAAI conference on artificial intelligence},
  year={2020}
}
```
```bibtex
@article{zhai2024looking,
  title={Looking into gait for perceiving emotions via bilateral posture and movement graph convolutional networks},
  author={Zhai, Yingjie and Jia, Guoli and Lai, Yu-Kun and Zhang, Jing and Yang, Jufeng and Tao, Dacheng},
  journal={IEEE Transactions on Affective Computing},
  volume={15},
  number={3},
  pages={1634--1648},
  year={2024},
}
```