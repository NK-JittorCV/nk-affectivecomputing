#  åŸºäºå›½äº§æ·±åº¦æ¡†æ¶Jittorè®¡å›¾çš„è®­ç»ƒä¸éƒ¨ç½²è§£å†³æ–¹æ¡ˆ

<p align="center">
    <br>
    <img src="assets/logo.png"/>
    <br>
<p>
<p align="center">
<a href="">é¡¹ç›®ä¸»é¡µ</a>
<br>
        ä¸­æ–‡&nbsp ï½œ &nbsp<a href="README.md">English</a>&nbsp
</p>
<p align="center">
<img src="https://img.shields.io/badge/python-3.8-5be.svg">
<img src="https://img.shields.io/badge/jittor-1.3.9-orange.svg">
<a href="https://github.com/zhongqihebut/Affective_Computing/blob/master/LICENSE"><img src="https://img.shields.io/github/license/zhongqihebut/Affective_Computing"></a>
<a href="https://github.com/zhongqihebut/Affective_Computing/pulls"><img src="https://img.shields.io/badge/PR-welcome-55EB99.svg"></a>
</p>

<p align="center">
        <a href="./docs/en/papers.md">ç›¸å…³è®ºæ–‡</a> &nbsp ï½œ <a href="./docs/en">English Documentation</a> &nbsp ï½œ &nbsp <a href="./docs/cn">ä¸­æ–‡æ–‡æ¡£</a> &nbsp
</p>

## ğŸ“– ç›®å½•
- [ç®€ä»‹](#-ç®€ä»‹)
- [æ–°é—»](#-æ–°é—»)
- [å®‰è£…](#%EF%B8%8F-å®‰è£…)
- [ä½¿ç”¨](#-ä½¿ç”¨)
- [License](#-License)
- [Citation](#-citation)

  



## ğŸ“ ç®€ä»‹

***æƒ…æ™ºå…¼å¤‡*** æ˜¯æ–°ä¸€ä»£äººå·¥æ™ºèƒ½çš„é‡è¦å‘å±•æ–¹å‘ï¼Œæ˜¯è¿ˆå‘é€šç”¨äººå·¥æ™ºèƒ½çš„å…³é”®ä¸€æ­¥ã€‚åœ¨äººæœºäº¤äº’åœºæ™¯ä¸­ï¼Œå…·å¤‡æƒ…æ™ºçš„æ•°å­—äººä¸æœºå™¨äººéœ€è¦ç²¾å‡†è§£è¯‘å¤šæ¨¡æ€äº¤äº’ä¿¡æ¯ï¼Œæ·±åº¦æŒ–æ˜äººç±»å†…åœ¨æƒ…æ„ŸçŠ¶æ€ï¼Œä»è€Œå®ç°æ›´å…·çœŸå®æ„Ÿä¸è‡ªç„¶æ€§çš„äººæœºå¯¹è¯ã€‚ç„¶è€Œï¼Œé¢å¯¹å¤šæ¨¡æ€æƒ…æ„Ÿæ•°æ®è¯­ä¹‰çš„é«˜åº¦å¤æ‚æ€§ï¼Œå¦‚ä½•æœ‰æ•ˆå»ºæ¨¡è·¨æ¨¡æ€å…³è”å…³ç³»ä»æ˜¯é¢†åŸŸå†…äºŸå¾…çªç ´çš„æ ¸å¿ƒæŒ‘æˆ˜ã€‚

JACKï¼ˆåŸºäºJittorçš„æƒ…æ„Ÿè®¡ç®—æ¨¡å‹è®­ç»ƒä¸éƒ¨ç½²æ¡†æ¶ï¼‰æ˜¯ç”±å—å¼€å¤§å­¦è®¡ç®—æœºè§†è§‰å›¢é˜Ÿæä¾›çš„å®˜æ–¹æ¡†æ¶ï¼ŒåŸºäºå›½äº§åŒ–é«˜æ€§èƒ½æ·±åº¦å­¦ä¹ æ¡†æ¶è®¡å›¾ï¼ˆJittorï¼‰è¿›è¡Œæƒ…æ„Ÿè®¡ç®—æ–¹æ³•çš„è®­ç»ƒä¸éƒ¨ç½²ã€‚ç›®å‰ï¼Œè¯¥æ¡†æ¶æ”¯æŒå…ˆè¿›çš„è§†é¢‘æƒ…æ„Ÿåˆ†ææ–¹æ³•ä»¥åŠæ­¥æ€è§†é¢‘æƒ…æ„Ÿåˆ†ææ–¹æ³•ã€‚åŸºäºJittorå›½äº§æ¡†æ¶ï¼Œæƒ…æ„Ÿè®¡ç®—æ–¹æ³•çš„éƒ¨ç½²é€Ÿåº¦ç›¸æ¯”PyTorchå¯æå‡1.1è‡³1.6å€ï¼Œä»è€Œæ”¯æŒä¸‹æ¸¸åº”ç”¨å¦‚æ¸¸å®¢æƒ…æ„Ÿæ£€æµ‹ã€å¯¹è¯åˆ†æã€èˆ†æƒ…ç›‘æ§ç­‰ã€‚



è¯¥é¡¹ç›®ç›®å‰æ”¯æŒåŸºäºJittoræ·±åº¦å­¦ä¹ æ¡†æ¶çš„æƒ…æ„Ÿè®¡ç®—é¢†åŸŸä¸­çš„å››é¡¹å·¥ä½œï¼š

| **å·¥ä½œ**| **è®­ç»ƒ** | **æµ‹è¯•** |
|-----------------------------------------------------------------------------------------------------------|-----------|----------|
| [[CTEN]](https://openaccess.thecvf.com/content/CVPR2023/papers/Zhang_Weakly_Supervised_Video_Emotion_Detection_and_Prediction_via_Cross-Modal_Temporal_CVPR_2023_paper.pdf) | [[Train]]() | [[Test]]() |
| [[TSL_Net]](https://github.com/nku-zhichengzhang/TSL300/blob/main/assests/acm22_zzc_videosenti_official.pdf) | [[Train]]() | [[Test]]() |
| [[VAANet]](https://arxiv.org/abs/2003.00832)                                                              | [[Train]]() | [[Test]]() |
| [[Gait]](https://ieeexplore.ieee.org/document/10433680)                                                   | [[Train]]() | [[Test]]() |


åœ¨æƒ…æ„Ÿè®¡ç®—é¢†åŸŸï¼ŒJittoré«˜æ€§èƒ½æ·±åº¦å­¦ä¹ æ¡†æ¶ä¸PyTorchæ¡†æ¶çš„æ€§èƒ½å¯¹æ¯”ï¼š

| **å·¥ä½œ**| **è®­ç»ƒ** | **æµ‹è¯•** |
|-----------------------------------------------------------------------------------------------------------|-----------|----------|
| [[CTEN]](https://openaccess.thecvf.com/content/CVPR2023/papers/Zhang_Weakly_Supervised_Video_Emotion_Detection_and_Prediction_via_Cross-Modal_Temporal_CVPR_2023_paper.pdf) | [[Train]]() | [[Test]]() |
| [[TSL_Net]](https://github.com/nku-zhichengzhang/TSL300/blob/main/assests/acm22_zzc_videosenti_official.pdf) | [[Train]]() | [[Test]]() |
| [[VAANet]](https://arxiv.org/abs/2003.00832)                                                              | [[Train]]() | [[Test]]() |
| [[Gait]](https://ieeexplore.ieee.org/document/10433680)                                                   | [[Train]]() | [[Test]]() |



## ğŸ‰ æ–°é—»
- ğŸ 2025.07.16: Project initialized. This project supports four video emotion analysis tasks, including [CTEN](https://openaccess.thecvf.com/content/CVPR2023/papers/Zhang_Weakly_Supervised_Video_Emotion_Detection_and_Prediction_via_Cross-Modal_Temporal_CVPR_2023_paper.pdf), [TSL_Net](https://github.com/nku-zhichengzhang/TSL300/blob/main/assests/acm22_zzc_videosenti_official.pdf), [VAANet](https://arxiv.org/abs/2003.00832), and [Gait](https://ieeexplore.ieee.org/document/10433680). Training and testing scripts are provided for all methods.


## ğŸ› ï¸ å®‰è£…
To install using pip:
```shell
pip install -r requirements.txt
```

Running Environment:

|              | Range        | Recommended | Notes                                     |
| ------------ |--------------| ----------- | ----------------------------------------- |
| python       | >=3.8        | 3.8        |                                           |
| cuda         |              | cuda11.3   | No need to install if using CPU, NPU, MPS |
| jittor       |        |  1.3.9.14     |                                           |
| transformers | >=4.30       | 4.31.0      |                                           |


For more optional dependencies, you can refer to [here](./docs/en/env.md).




## âœ¨ ä½¿ç”¨
Here is a minimal example of training and deployment using JACK.

- If you want to use other models or datasets (including multimodal models and datasets), you only need to modify `--model` to specify the corresponding model's ID or path, and modify `--dataset` to specify the corresponding dataset's ID or path.

|   Useful Links |
| ------ |
|   [ğŸ”¥Supported Methods](./docs/en/papers.md)   |
|   [Train](./docs/en/train.md)   |
|   [Test](./docs/en/test.md) |
|   [Datasets](./docs/en/dataset.md)   |
|   [Torch2Jittor FAQ](./docs/en/FAQ.md)   |



## ğŸ› License

æœ¬æ¡†æ¶ä½¿ç”¨[Apache License (Version 2.0)](https://github.com/modelscope/modelscope/blob/master/LICENSE)è¿›è¡Œè®¸å¯ã€‚æ¨¡å‹å’Œæ•°æ®é›†è¯·æŸ¥çœ‹åŸèµ„æºé¡µé¢å¹¶éµå®ˆå¯¹åº”Licenseã€‚


## ğŸ“ å¼•ç”¨

```bibtex
@inproceedings{zhang2025moda,
  author = {Zhang, Zhicheng and Xia, Wuyou and Zhao, Chenxi and Yan, Zhou and Liu, Xiaoqiang and Zhu, Yongjie and Qin, Wenyu and Wan, Pengfei and Zhang, Di and Yang, Jufeng},
  title = {MODA: MOdular Duplex Attention for Multimodal Perception, Cognition, and Emotion Understanding},
  booktitle = {Proceedings of the 42nd International Conference on Machine Learning (ICML)},
  year = {2025},
}
```
```bibtex
@inproceedings{zhang2024masked,
  author = {Zhang, Zhicheng and Zhao, Pancheng and Park, Eunil and Yang, Jufeng},
  title = {MART: Masked Affective RepresenTation Learning via Masked Temporal Distribution Distillation},
  booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
  year = {2024},
}
```

```bibtex
@inproceedings{Zhang_2023_CVPR,
  author = {Zhang, Zhicheng and Wang, Lijuan and Yang, Jufeng},
  title = {Weakly Supervised Video Emotion Detection and Prediction via Cross-Modal Temporal Erasing Network},
  booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
  year = {2023},
}
```
```bibtex
@inproceedings{10.1145/3503161.3548007,
  author = {Zhang, Zhicheng and Yang, Jufeng},
  title = {Temporal Sentiment Localization: Listen and Look in Untrimmed Videos},
  year = {2022},
  booktitle = {Proceedings of the 30th ACM International Conference on Multimedia},
}
```
```bibtex
@inproceedings{zhao2020end,
  title={An end-to-end visual-audio attention network for emotion recognition in user-generated videos},
  author={Zhao, Sicheng and Ma, Yunsheng and Gu, Yang and Yang, Jufeng and Xing, Tengfei and Xu, Pengfei and Hu, Runbo and Chai, Hua and Keutzer, Kurt},
  booktitle={Proceedings of the AAAI conference on artificial intelligence},
  year={2020}
}
```
```bibtex
@article{zhai2024looking,
  title={Looking into gait for perceiving emotions via bilateral posture and movement graph convolutional networks},
  author={Zhai, Yingjie and Jia, Guoli and Lai, Yu-Kun and Zhang, Jing and Yang, Jufeng and Tao, Dacheng},
  journal={IEEE Transactions on Affective Computing},
  volume={15},
  number={3},
  pages={1634--1648},
  year={2024},
}
```